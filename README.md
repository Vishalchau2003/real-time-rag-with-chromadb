# Real-Time RAG App with ChromaDB & Ollama

## ğŸ“Œ Project Overview
This is a **Real-Time Retrieval-Augmented Generation (RAG) application** that:
- **Ingests new text files** in real time.
- **Stores document embeddings in ChromaDB**.
- **Retrieves relevant documents** for user queries.
- **Generates AI responses using a local model** (Llama 3 or Mistral via Ollama).

## ğŸš€ Features
âœ… **Real-time data ingestion** (auto-detects new files in `data/` folder)
âœ… **Fast retrieval** using ChromaDB
âœ… **AI-generated answers** powered by a local LLM (Llama 3 / Mistral via Ollama)
âœ… **Interactive Web UI** built with Streamlit
âœ… **Completely offline (No OpenAI API needed)**

---

## ğŸ› ï¸ Installation & Setup
### **1ï¸âƒ£ Clone the Repository**
```sh
git clone https://github.com/your-username/real-time-rag-app.git
cd real-time-rag-app
```

### **2ï¸âƒ£ Create & Activate Virtual Environment**
```sh
python -m venv env  # Create virtual environment
source env/bin/activate  # Mac/Linux
env\Scripts\activate  # Windows
```

### **3ï¸âƒ£ Install Dependencies**
```sh
pip install -r requirements.txt
```

### **4ï¸âƒ£ Install Ollama & a Local Model**
1. Download & Install Ollama: [https://ollama.com](https://ollama.com)
2. Pull a model (choose one):
   ```sh
   ollama pull llama3   # Llama 3 model
   ollama pull mistral  # Mistral model
   ```

---

## ğŸš€ Running the App
### **1ï¸âƒ£ Start Real-Time Ingestion**
```sh
python ingest.py
```
ğŸ“‚ **Add new text files to `data/` folder** â€“ they will be automatically indexed!

### **2ï¸âƒ£ Query the Data in Terminal**
```sh
python vector_store.py
```
ğŸ’¬ **Enter your question**, and the system will retrieve relevant documents & generate an AI response using Ollama!

### **3ï¸âƒ£ Run the Web UI**
```sh
streamlit run app.py
```
ğŸŒ Open `http://localhost:8501` in your browser to use the interactive UI.

---

## ğŸ“ Folder Structure
```
real-time-rag-app/
â”‚â”€â”€ data/                  # Folder for text documents
â”‚â”€â”€ env/                   # Virtual environment (not uploaded)
â”‚â”€â”€ vector_db/             # ChromaDB vector database (not uploaded)
â”‚â”€â”€ ingest.py              # Real-time ingestion script
â”‚â”€â”€ vector_store.py        # Retrieval and AI response script
â”‚â”€â”€ app.py                 # Streamlit Web UI
â”‚â”€â”€ requirements.txt       # Dependencies
â”‚â”€â”€ README.md              # Project documentation
â”‚â”€â”€ .gitignore             # Ignore unnecessary files
```

---

## ğŸ“Œ Notes
- **This project runs fully offline** (no OpenAI API required).
- **Replace `llama3` with `mistral`** in `vector_store.py` and `app.py` if you prefer Mistral.
- **Make sure Ollama is running before testing AI responses!**



